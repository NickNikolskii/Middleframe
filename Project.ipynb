{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading https://files.pythonhosted.org/packages/1f/51/e0b9cef23098bc31c77b0e06221dd8d05119b9782d4c2b1d1482e22b5f5e/opencv_python-4.1.1.26-cp37-cp37m-win_amd64.whl (39.0MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in d:\\conda\\lib\\site-packages (from opencv-python) (1.16.4)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.1.1.26\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')\n",
    "\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "batch_size = 128\n",
    "learning_rate = 1e-3\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#     transforms.Grayscale(num_output_channels=1),\n",
    "])\n",
    "\n",
    "dataset = MNIST('./data', download=True, transform=img_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 28 * 28), nn.Tanh())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = autoencoder()#.cuda()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(num_epochs):\n",
    "#     for data in dataloader:\n",
    "#         img, _ = data\n",
    "#         #print(img.shape)\n",
    "#         img = img.view(img.size(0), -1)\n",
    "#         img = Variable(img)#.cuda()\n",
    "#         output = model(img)\n",
    "#         loss = criterion(output, img)\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#     print('epoch [{}/{}], loss:{:.4f}'\n",
    "#           .format(epoch + 1, num_epochs, loss.item()))\n",
    "#     if epoch % 10 == 0:\n",
    "#         pic = to_img(output.cpu().data)\n",
    "#         save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "\n",
    "        \n",
    "# torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model, './sim_autoencoder.pth')\n",
    "model.load_state_dict(torch.load('./sim_autoencoder.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#model = torch.load('./sim_autoencoder.pth', map_location='cpu')\n",
    "img, _ = dataset[0]\n",
    "#print(img)\n",
    "my_img = model(img.reshape(28*28))\n",
    "my_img = my_img.reshape(1,28,28)\n",
    "#my_img.shape\n",
    "results = transforms.ToPILImage(mode='L')(my_img)\n",
    "results.show()\n",
    "#output[0]\n",
    "#image = output.cpu().numpy()[0]\n",
    "#image = np.transpose(image, (1,2,0))\n",
    "#plt.matshow(image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = dataset[0]\n",
    "results = transforms.ToPILImage(mode='L')(img.reshape(1,28,28)) \n",
    "results.show()\n",
    "\n",
    "img2 = model(img.reshape(28*28))\n",
    "results2 = transforms.ToPILImage(mode='L')(img2.reshape(1,28,28)) \n",
    "results2.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show middle\n",
    "img, _ = dataset[0]\n",
    "img2, _ = dataset[1]\n",
    "enc1 = model.encoder(img.reshape(28*28))\n",
    "enc2 = model.encoder(img2.reshape(28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (enc1+enc2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toPILimg(img):\n",
    "    return transforms.ToPILImage(mode='L')(img.reshape(1,28,28))\n",
    "\n",
    "def encode(x):\n",
    "    form_img = x.reshape(28*28)\n",
    "    return model.encoder(form_img)\n",
    "\n",
    "def decode(enc):\n",
    "    dec_img = model.decoder(enc)\n",
    "    return toPILimg(dec_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = encode_img(img2)\n",
    "y = decode_enc(x)\n",
    "y.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "img5_2, _ = dataset[11]\n",
    "img5_1, _ = dataset[0]\n",
    "toPILimg(img5_1).show()\n",
    "toPILimg(img5_2).show()\n",
    "decode((encode(img5_1) + encode(img5_2))/2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
