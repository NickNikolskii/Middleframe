{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Middle Frame Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Preparaion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Autoencoder Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 28 * 28), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) VAE Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 12)\n",
    "        self.fc41 = nn.Linear(12, 3)\n",
    "        self.fc42 = nn.Linear(12, 3)\n",
    "        self.fc5 = nn.Linear(3, 12)\n",
    "        self.fc6 = nn.Linear(12, 64)\n",
    "        self.fc7 = nn.Linear(64, 128)\n",
    "        self.fc8 = nn.Linear(128, 28 * 28)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h1 = self.relu(self.fc1(x))\n",
    "        h2 = self.relu(self.fc2(h1))\n",
    "        h3 = self.relu(self.fc3(h2))\n",
    "        return self.fc41(h3), self.fc42(h3)\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def decode(self, z):\n",
    "        h4 = self.relu(self.fc5(z))\n",
    "        h5 = self.relu(self.fc6(h4))\n",
    "        h6 = self.relu(self.fc7(h5))\n",
    "        return self.sigmoid(self.fc8(h6))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#     transforms.Grayscale(num_output_channels=1),\n",
    "])\n",
    "\n",
    "\n",
    "image_folder = './data'\n",
    "\n",
    "# dataset = ImageFolder('./data', trans..=...)\n",
    "dataset = MNIST(image_folder, download=True, transform=img_transform)\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "data_iter = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Network Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test = train_test_split(dataset, test_size=0.33, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter_train = DataLoader(X_train, batch_size = batch_size, shuffle = True)\n",
    "data_iter_test = DataLoader(X_test, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = autoencoder()#.cuda()\n",
    "model2 = VAE()\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "\n",
    "learning_rate = 1e-3\n",
    "#optimizer = torch.optim.Adam(\n",
    "#    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model2.parameters(), lr=learning_rate, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/200], loss:0.0549\n",
      "epoch [2/200], loss:0.0494\n",
      "epoch [3/200], loss:0.0462\n",
      "epoch [4/200], loss:0.0432\n",
      "epoch [5/200], loss:0.0416\n",
      "epoch [6/200], loss:0.0407\n",
      "epoch [7/200], loss:0.0394\n",
      "epoch [8/200], loss:0.0387\n",
      "epoch [9/200], loss:0.0381\n",
      "epoch [10/200], loss:0.0376\n",
      "epoch [11/200], loss:0.0373\n",
      "epoch [12/200], loss:0.0370\n",
      "epoch [13/200], loss:0.0366\n",
      "epoch [14/200], loss:0.0363\n",
      "epoch [15/200], loss:0.0360\n",
      "epoch [16/200], loss:0.0360\n",
      "epoch [17/200], loss:0.0359\n",
      "epoch [18/200], loss:0.0356\n",
      "epoch [19/200], loss:0.0355\n",
      "epoch [20/200], loss:0.0355\n",
      "epoch [21/200], loss:0.0354\n",
      "epoch [22/200], loss:0.0351\n",
      "epoch [23/200], loss:0.0351\n",
      "epoch [24/200], loss:0.0348\n",
      "epoch [25/200], loss:0.0349\n",
      "epoch [26/200], loss:0.0346\n",
      "epoch [27/200], loss:0.0347\n",
      "epoch [28/200], loss:0.0344\n",
      "epoch [29/200], loss:0.0344\n",
      "epoch [30/200], loss:0.0344\n",
      "epoch [31/200], loss:0.0341\n",
      "epoch [32/200], loss:0.0344\n",
      "epoch [33/200], loss:0.0340\n",
      "epoch [34/200], loss:0.0342\n",
      "epoch [35/200], loss:0.0341\n",
      "epoch [36/200], loss:0.0338\n",
      "epoch [37/200], loss:0.0339\n",
      "epoch [38/200], loss:0.0338\n",
      "epoch [39/200], loss:0.0338\n",
      "epoch [40/200], loss:0.0336\n",
      "epoch [41/200], loss:0.0336\n",
      "epoch [42/200], loss:0.0337\n",
      "epoch [43/200], loss:0.0334\n",
      "epoch [44/200], loss:0.0337\n",
      "epoch [45/200], loss:0.0334\n",
      "epoch [46/200], loss:0.0335\n",
      "epoch [47/200], loss:0.0334\n",
      "epoch [48/200], loss:0.0333\n",
      "epoch [49/200], loss:0.0333\n",
      "epoch [50/200], loss:0.0334\n",
      "epoch [51/200], loss:0.0332\n",
      "epoch [52/200], loss:0.0332\n",
      "epoch [53/200], loss:0.0332\n",
      "epoch [54/200], loss:0.0334\n",
      "epoch [55/200], loss:0.0332\n",
      "epoch [56/200], loss:0.0331\n",
      "epoch [57/200], loss:0.0334\n",
      "epoch [58/200], loss:0.0332\n",
      "epoch [59/200], loss:0.0330\n",
      "epoch [60/200], loss:0.0330\n",
      "epoch [61/200], loss:0.0328\n",
      "epoch [62/200], loss:0.0330\n",
      "epoch [63/200], loss:0.0329\n",
      "epoch [64/200], loss:0.0327\n",
      "epoch [65/200], loss:0.0327\n",
      "epoch [66/200], loss:0.0329\n",
      "epoch [67/200], loss:0.0328\n",
      "epoch [68/200], loss:0.0333\n",
      "epoch [69/200], loss:0.0328\n",
      "epoch [70/200], loss:0.0329\n",
      "epoch [71/200], loss:0.0328\n",
      "epoch [72/200], loss:0.0326\n",
      "epoch [73/200], loss:0.0329\n",
      "epoch [74/200], loss:0.0327\n",
      "epoch [75/200], loss:0.0326\n",
      "epoch [76/200], loss:0.0327\n",
      "epoch [77/200], loss:0.0326\n",
      "epoch [78/200], loss:0.0325\n",
      "epoch [79/200], loss:0.0325\n",
      "epoch [80/200], loss:0.0324\n",
      "epoch [81/200], loss:0.0324\n",
      "epoch [82/200], loss:0.0324\n",
      "epoch [83/200], loss:0.0324\n",
      "epoch [84/200], loss:0.0329\n",
      "epoch [85/200], loss:0.0326\n",
      "epoch [86/200], loss:0.0323\n",
      "epoch [87/200], loss:0.0324\n",
      "epoch [88/200], loss:0.0323\n",
      "epoch [89/200], loss:0.0326\n",
      "epoch [90/200], loss:0.0324\n",
      "epoch [91/200], loss:0.0322\n",
      "epoch [92/200], loss:0.0323\n",
      "epoch [93/200], loss:0.0324\n",
      "epoch [94/200], loss:0.0324\n",
      "epoch [95/200], loss:0.0325\n",
      "epoch [96/200], loss:0.0324\n",
      "epoch [97/200], loss:0.0323\n",
      "epoch [98/200], loss:0.0321\n",
      "epoch [99/200], loss:0.0322\n",
      "epoch [100/200], loss:0.0322\n",
      "epoch [101/200], loss:0.0322\n",
      "epoch [102/200], loss:0.0322\n",
      "epoch [103/200], loss:0.0325\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-904b0c742019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#.to(device)#.cuda()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m#output = model(input)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-8a4166b1aeb8>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m784\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreparameterize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-25-8a4166b1aeb8>\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, z)\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mh5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc6\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mh6\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc7\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfc8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 547\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    548\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Conda\\lib\\site-packages\\torch\\nn\\modules\\activation.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "testdataset = [dataset[1][0].reshape(1,28,28),\n",
    "               dataset[3][0].reshape(1,28,28),\n",
    "               dataset[5][0].reshape(1,28,28),\n",
    "               dataset[7][0].reshape(1,28,28),\n",
    "               dataset[2][0].reshape(1,28,28),\n",
    "               dataset[0][0].reshape(1,28,28),\n",
    "               dataset[13][0].reshape(1,28,28),\n",
    "               dataset[38][0].reshape(1,28,28),\n",
    "               dataset[17][0].reshape(1,28,28),\n",
    "               dataset[4][0].reshape(1,28,28)]\n",
    "\n",
    "#(dataset[1]) #0\n",
    "#(dataset[3]) #1\n",
    "#(dataset[5]) #2\n",
    "#(dataset[7]) #3\n",
    "#(dataset[2]) #4\n",
    "#(dataset[0]) #5\n",
    "#(dataset[13]) #6\n",
    "#(dataset[38]) #7\n",
    "#(dataset[17]) #8\n",
    "#(dataset[4]) #9\n",
    "\n",
    "temp = torchvision.utils.make_grid(testdataset, nrow=5)\n",
    "temp = temp[0]\n",
    "#print(temp.shape)\n",
    "writer.add_image(\"Test Dataset\", temp.reshape(1,62,152))\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.clamp(0, 1)\n",
    "    x = x.view(x.size(0), 1, 28, 28)\n",
    "    return x\n",
    "\n",
    "\n",
    "num_epochs = 200\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    total_tr_loss = 0\n",
    "    n_tr = 0\n",
    "    total_te_loss = 0\n",
    "    n_te = 0\n",
    "    \n",
    "    for data in data_iter_train:\n",
    "        img, _ = data\n",
    "        \n",
    "        input = img.view(img.size(0), -1)#.to(device)#.cuda()\n",
    "        #output = model(input)\n",
    "        output = model2(input)[0]\n",
    "        \n",
    "        loss = loss_func(output, input)\n",
    "        total_tr_loss += loss.item()\n",
    "        n_tr += 1\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        for data in data_iter_test:\n",
    "            img_t, _ = data\n",
    "        \n",
    "            input = img_t.view(img_t.size(0), -1)#.to(device)#.cuda()\n",
    "            #output = model(input)\n",
    "            output = model2(input)[0]\n",
    "        \n",
    "            loss = loss_func(output, input)\n",
    "            total_te_loss += loss.item()\n",
    "            n_te += 1\n",
    "    \n",
    "    mean_te_loss = total_te_loss/n_te\n",
    "    mean_tr_loss = total_tr_loss/n_tr\n",
    "    \n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "          .format(epoch + 1, num_epochs, mean_te_loss))\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        pic = to_img(output.cpu().data)\n",
    "        save_image(pic, './mlp_img/image_{}.png'.format(epoch))\n",
    "    \n",
    "    temp = torchvision.utils.make_grid([input[0].reshape(1,28,28),\n",
    "                    input[1].reshape(1,28,28),\n",
    "                    input[2].reshape(1,28,28),\n",
    "                    input[3].reshape(1,28,28),\n",
    "                    input[4].reshape(1,28,28),\n",
    "                    input[5].reshape(1,28,28),\n",
    "                    input[7].reshape(1,28,28),\n",
    "                    input[8].reshape(1,28,28)])\n",
    "    \n",
    "    temp = temp[0]\n",
    "    \n",
    "    writer.add_image(\"Training data\", temp.reshape(1,32,242), epoch)\n",
    "    \n",
    "    temp = torchvision.utils.make_grid([output[0].reshape(1,28,28),\n",
    "                    output[1].reshape(1,28,28),\n",
    "                    output[2].reshape(1,28,28),\n",
    "                    output[3].reshape(1,28,28),\n",
    "                    output[4].reshape(1,28,28),\n",
    "                    output[5].reshape(1,28,28),\n",
    "                    output[7].reshape(1,28,28),\n",
    "                    output[8].reshape(1,28,28)])\n",
    "    \n",
    "    temp = temp[0]\n",
    "    test_output = []\n",
    "    \n",
    "    for data in testdataset:\n",
    "        img = data\n",
    "        \n",
    "        input = img.view(img.size(0), -1)#.to(device)#.cuda()\n",
    "        #test_output.append(model(input)[0])\n",
    "        test_output.append(model2(input)[0])\n",
    "        \n",
    "    temp2 = torchvision.utils.make_grid([test_output[0].reshape(1,28,28),\n",
    "                    test_output[1].reshape(1,28,28),\n",
    "                    test_output[2].reshape(1,28,28),\n",
    "                    test_output[3].reshape(1,28,28),\n",
    "                    test_output[4].reshape(1,28,28),\n",
    "                    test_output[5].reshape(1,28,28),\n",
    "                    test_output[6].reshape(1,28,28),\n",
    "                    test_output[7].reshape(1,28,28),\n",
    "                    test_output[8].reshape(1,28,28),\n",
    "                    test_output[9].reshape(1,28,28)], nrow=5)\n",
    "\n",
    "    writer.add_image(\"Model Test Dataset\", temp2[0].reshape(1,62,152), epoch)\n",
    "    writer.add_image(\"Output\", temp.reshape(1,32,242), epoch)\n",
    "    writer.add_scalar(\"Loss/Train\", mean_tr_loss, epoch)\n",
    "    writer.add_scalar(\"Loss/Test\", mean_te_loss, epoch)\n",
    "    writer.add_scalars(\"Loss\", {\"Train\": mean_tr_loss, \"Test\": mean_te_loss}, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Loading And Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './sim_autoencoder.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.save(model, './sim_autoencoder.pth')\n",
    "model.load_state_dict(torch.load('./sim_autoencoder.pth', map_location='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying to Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forshow(img):\n",
    "    return transforms.ToPILImage(mode='L')(img.reshape(28,28))\n",
    "\n",
    "def encode(img_from_dataset):\n",
    "    norm_img = img_from_dataset.reshape(28*28)\n",
    "    return model.encoder(norm_img)\n",
    "\n",
    "def decode(encoding):\n",
    "    dec_img = model.decoder(encoding)\n",
    "    return dec_img\n",
    "\n",
    "def midFrame(img1, img2):\n",
    "    return decode((encode(img1) + encode(img2))/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = dataset[0][0]\n",
    "im2 = dataset[6][0]\n",
    "\n",
    "forshow(im1).show()\n",
    "forshow(decode(encode(im1))).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mid1 = midFrame(im1, im2)\n",
    "\n",
    "mid2 = midFrame(mid1,im2)\n",
    "\n",
    "mid3 = midFrame(im1, mid1)\n",
    "for i in [im1, mid3, mid1, mid2, im2]:\n",
    "    forshow(i).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = dataset[5]\n",
    "\n",
    "my_img = model(img.reshape(28*28))\n",
    "my_img = my_img.reshape(1,28,28)\n",
    "\n",
    "results = transforms.ToPILImage(mode='L')(my_img)\n",
    "results.show()\n",
    "\n",
    "#output[0]\n",
    "#image = output.cpu().numpy()[0]\n",
    "#image = np.transpose(image, (1,2,0))\n",
    "#plt.matshow(image)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "img, _ = dataset[5]\n",
    "results = transforms.ToPILImage(mode='L')(img.reshape(1,28,28)) \n",
    "results.show()\n",
    "\n",
    "img2 = model(img.reshape(28*28))\n",
    "results2 = transforms.ToPILImage(mode='L')(img2.reshape(1,28,28)) \n",
    "results2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show middle\n",
    "img, _ = dataset[0]\n",
    "img2, _ = dataset[1]\n",
    "enc1 = model.encoder(img.reshape(28*28))\n",
    "enc2 = model.encoder(img2.reshape(28*28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "toPILimg(img).show()\n",
    "toPILimg(img2).show()\n",
    "result = (enc1+enc2)/2\n",
    "decode(result).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'toPILimg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-c87949b37299>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mimg5_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg5_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtoPILimg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg5_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtoPILimg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg5_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmidFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg5_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg5_2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'toPILimg' is not defined"
     ]
    }
   ],
   "source": [
    "img5_2, _ = dataset[11]\n",
    "img5_1, _ = dataset[0]\n",
    "toPILimg(img5_1).show()\n",
    "toPILimg(img5_2).show()\n",
    "midFrame(img5_1, img5_2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
