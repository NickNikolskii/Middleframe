{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: No imports in the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data loaded\n",
      "data split\n",
      "training started\n",
      "model improved\n",
      "test started\n",
      "test complete\n",
      "visualisation started\n",
      "Loss: 0.00040049731072903884\n",
      "test 0 visualized\n",
      "model improved\n",
      "test started\n",
      "test complete\n",
      "visualisation started\n",
      "Loss: 0.00033346586757236054\n",
      "test 1 visualized\n",
      "model improved\n",
      "test started\n",
      "test complete\n",
      "visualisation started\n",
      "Loss: 0.00030512038919359747\n",
      "test 2 visualized\n",
      "model improved\n",
      "test started\n",
      "test complete\n",
      "visualisation started\n",
      "Loss: 0.0002879559561008155\n",
      "test 3 visualized\n",
      "model improved\n",
      "test started\n",
      "test complete\n",
      "visualisation started\n",
      "Loss: 0.00027637306920657255\n",
      "test 4 visualized\n"
     ]
    }
   ],
   "source": [
    "data_folder = './data'\n",
    "dataset = load_imgs(data_folder)\n",
    "print('data loaded')\n",
    "\n",
    "model = autoencoder_conv()\n",
    "\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.33, random_state=1)\n",
    "\n",
    "print('data split')\n",
    "\n",
    "optimizations = opts()\n",
    "writer = SummaryWriter()\n",
    "print('training started')\n",
    "train(model, train_data, *optimizations, num_epochs = 5, test_data = test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_conv, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 3, stride=3, padding=1),  # b, 16, 10, 10\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=2),  # b, 16, 5, 5\n",
    "            nn.Conv2d(16, 8, 3, stride=2, padding=1),  # b, 8, 3, 3\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, stride=1)  # b, 8, 2, 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(8, 16, 3, stride=2),  # b, 16, 5, 5\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 5, stride=3, padding=1),  # b, 8, 15, 15\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 2, stride=2, padding=1),  # b, 1, 28, 28\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def improve(self, train_data_iter, optimizer, loss_func):\n",
    "        for imgs, _ in train_data_iter:\n",
    "            input = imgs\n",
    "            output = self(imgs)\n",
    "            \n",
    "            loss = loss_func(input, output)\n",
    "            optimize(optimizer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class autoencoder_lin(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(autoencoder_lin, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            View((-1, 28*28)),\n",
    "            nn.Linear(28 * 28, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True), nn.Linear(64, 12), nn.ReLU(True), nn.Linear(12, 3))\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(3, 12),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(12, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(True), nn.Linear(128, 28 * 28), nn.Tanh(),\n",
    "            View((-1, 1, 28, 28)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def improve(self, train_data_iter, optimizer, loss_func):\n",
    "        for imgs, _ in train_data_iter:\n",
    "            input = imgs\n",
    "            output = self(imgs)\n",
    "            \n",
    "            loss = loss_func(input, output)\n",
    "            optimize(optimizer, loss)\n",
    "            \n",
    "class View(nn.Module):\n",
    "    def __init__(self, shape):\n",
    "        super(View, self).__init__()\n",
    "        self.shape = shape\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x.view(*self.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(img_folder, iterator = True):\n",
    "    img_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # dataset = ImageFolder('./data', trans..=...)\n",
    "    dataset = MNIST(img_folder, download=True, transform=img_transform)\n",
    "    \n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def opts():\n",
    "    learning_rate = 1e-3\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    loss_func = nn.MSELoss()\n",
    "    return (optimizer, loss_func)\n",
    "\n",
    "\n",
    "def optimize(optimizer, loss):\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    \n",
    "def create_grid_from(data, size, n_vis):\n",
    "    grid = []\n",
    "    for j in range(n_vis):\n",
    "        el = data[j].reshape(*size)\n",
    "        grid.append(el)\n",
    "        \n",
    "    return torchvision.utils.make_grid(grid)\n",
    "\n",
    "    \n",
    "def plot_add(data, name, epoch):\n",
    "    temp = create_grid_from(data, (1,28,28), 8)\n",
    "    writer.add_image(name, temp, epoch)\n",
    "    \n",
    "    \n",
    "def apply_model(model, dataset):\n",
    "    new_dataset = []\n",
    "    \n",
    "    for el in dataset:\n",
    "        new_el = model(el)\n",
    "        new_dataset.append(new_el)\n",
    "        \n",
    "    return new_dataset\n",
    "    \n",
    "def visualize(mean_test_loss, inp, outp, vis_info):\n",
    "    ep = vis_info[0]\n",
    "    plot_add(inp, \"Input\", ep)\n",
    "    plot_add(outp, \"Output\", ep)\n",
    "    #tested = apply_model(model, test_dataset)\n",
    "    #plot_add_8(tested)\n",
    "    writer.add_scalar(\"Loss/Test\", mean_test_loss, ep)\n",
    "    \n",
    "    \n",
    "\n",
    "def test(model, test_data_iter, loss_func, vis_info):\n",
    "    total_test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for test_imgs, _ in test_data_iter:\n",
    "            input = test_imgs\n",
    "            output = model(input)\n",
    "        \n",
    "            loss = loss_func(input, output)\n",
    "            total_test_loss += loss.item()\n",
    "        \n",
    "        mean_test_loss = total_test_loss/len(test_data)\n",
    "    print('test complete')\n",
    "    print('visualisation started')\n",
    "    visualize(mean_test_loss, input, output, vis_info)\n",
    "    print('Loss:', mean_test_loss)\n",
    "    print('test',vis_info[0],'visualized')\n",
    "    \n",
    "    \n",
    "def train(model, data, optimizer, loss_func, num_epochs = 100, test_data = False):\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        data_iter = DataLoader(data, batch_size = 128)\n",
    "        model.improve(data_iter, optimizer, loss_func)\n",
    "        print('model improved')\n",
    "        if test_data:\n",
    "            print('test started')\n",
    "            test_data_iter = DataLoader(test_data, batch_size = 128)\n",
    "            vis_info = [epoch]\n",
    "            test(model, test_data_iter, loss_func, vis_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# needed_libs.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import save_image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "if not os.path.exists('./mlp_img'):\n",
    "    os.mkdir('./mlp_img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
